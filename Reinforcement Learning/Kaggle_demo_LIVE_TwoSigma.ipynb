{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "j4GZ6WbfjYDo",
    "outputId": "7f9c2643-9e4f-4282-fdf7-b63afc84a87b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TkAyj5NlMyTa"
   },
   "source": [
    "### Time series data Forecasting techniques\n",
    "\n",
    "#### The Naive Approach\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/naive_new-768x495.png)\n",
    "\n",
    "- If we want to forecast the price for the next day, we can simply take the last day value and estimate the same value for the next day.\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/Screen-Shot-2018-01-25-at-7.45.20-PM.png)\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/naive-768x519.png\n",
    ")\n",
    "\n",
    "#### The Simple average\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/avg_orig_new1-768x510.png)\n",
    "\n",
    "- Lots of times we are provided with a dataset, which varies by a small margin throughout it’s time period, but the average at each time period remains constant. \n",
    "- In such a case we can forecast the price of the next day somewhere similar to the average of all the past days.\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/Screen-Shot-2018-01-25-at-7.45.10-PM-300x82.png)\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/avg-768x511.png)\n",
    "\n",
    "\n",
    "#### The Moving average\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/mov_avg_new-768x531.png)\n",
    "\n",
    "- Using the prices of the initial period would highly affect the forecast for the next period. \n",
    "- Therefore as an improvement over simple average, we will take the average of the prices for last few time periods only. \n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/Screen-Shot-2018-01-25-at-7.47.33-PM.png)\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/moving_avg-850x428.png)\n",
    "\n",
    "\n",
    "#### Single Exponential smoothing \n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/Screen-Shot-2018-01-25-at-7.59.27-PM-768x90.png\n",
    ")\n",
    "\n",
    "-It may be sensible to attach larger weights to more recent observations than to observations from the distant past. \n",
    "- The technique which works on this principle is called Simple exponential smoothing.\n",
    "- Forecasts are calculated using weighted averages where the weights decrease exponentially as observations come from further in the past, the smallest weights are associated with the oldest observations:\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/SES-768x392.png)\n",
    "\n",
    "Doesn't this look similar to the discounted return concept from reinforcement learning? \n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/reinforcementlearning-170329091514/95/reinforcement-learning-17-638.jpg?cb=1490778934\n",
    ")\n",
    "\n",
    "#### Holt’s linear trend method\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/hl_new-768x408.png)\n",
    "\n",
    "- Holt extended simple exponential smoothing to allow forecasting of data with a trend.\n",
    "- It is nothing more than exponential smoothing applied to both level(the average value in the series) and trend.\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/eq-768x317.png)\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/HL-768x390.png)\n",
    "\n",
    "\n",
    "#### Holt’s Winter seasonal method\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/Picture1.jpg)\n",
    "\n",
    "- The level equation shows a weighted average between the seasonally adjusted observation and the non-seasonal forecast for time t. \n",
    "- The trend equation is identical to Holt’s linear method. \n",
    "- The seasonal equation shows a weighted average between the current seasonal index, and the seasonal index of the same season last year (i.e., s time periods ago).\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/eq.png)\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/02/HW.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-mnpA3CkClq"
   },
   "source": [
    "#### Multivariate Time Series\n",
    "\n",
    "- Similar, but finds linear interdependencies between multiple variables. \n",
    "- Each variable has a regression like equation, where it is regressed against its own lagged values and the lagged values of other variables\n",
    "- Examples includes ARIMA, ARIMAX, etc.\n",
    "- Or treat it like a supervised problem and use the sequential power of LSTM networks. \n",
    "\n",
    "#### Reinforcement Learning for time series data?\n",
    "\n",
    "- In reinforcement learning, there is an agent acting on the outside world, observing effects and learning to improve its behaviour. \n",
    "- In contrast, a time series forecast setting has a passive observer which do not interact with the environment. \n",
    "- Ideally, the environment 'reacts' to actions that an 'agent' takes. \n",
    "\n",
    "### Examples\n",
    "- Game world example is 'cartpole', if the pole moves in one direction, the balance its on moves as well.\n",
    "- Real world examples include any system that adapts to changes made by an agent\n",
    "- In the stock market, where buy and sell actions effect the price\n",
    "- In an electricity grid, where optimizing energy output effects interdependent variables like the cooling demand\n",
    "- Sensor networks, public works, all sorts of routing strategies inside of an interconnected system\n",
    "\n",
    "![alt text](http://www.turingfinance.com/wp-content/uploads/2014/04/Reinforcement-Learning.png)\n",
    "\n",
    "### We need more simulated environments! Create simulated environments for businesses, great startup idea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2PW9fbKelQHx"
   },
   "outputs": [],
   "source": [
    "#1 - List the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pwfve2u4lezY"
   },
   "outputs": [],
   "source": [
    "#2 - Convert data to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BRYU_7nlkcI"
   },
   "outputs": [],
   "source": [
    "#3 - how big is our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTDdw0LolpfQ"
   },
   "outputs": [],
   "source": [
    "#4 - Examine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMiZUVxKGndk"
   },
   "outputs": [],
   "source": [
    "#5 - how many labels and values do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-_7thzsB4ob"
   },
   "outputs": [],
   "source": [
    "#6 - How much missing data do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f982CK5GIoPT"
   },
   "outputs": [],
   "source": [
    "#7 - Counts for each timestep in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlMxT-1OI19s"
   },
   "outputs": [],
   "source": [
    "#8 - Unique Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_W5bNeGprGz"
   },
   "source": [
    "#### The Markov Decision Process\n",
    "\n",
    "![alt text](https://slideplayer.com/slide/3007502/11/images/5/Markov+Decision+Processes+%28MDPs%29.jpg)\n",
    "\n",
    "- States (observations) - 1 variable (target)\n",
    "- Actions - Iterate 1\n",
    "- Rewards - R score (loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5D6NnGCl3m4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# This is taken from Frans Slothoubers post on the contest discussion forum.\n",
    "# https://www.kaggle.com/slothouber/two-sigma-financial-modeling/kagglegym-emulation\n",
    "\n",
    "\n",
    "def r_score(y_true, y_pred, sample_weight=None, multioutput=None):\n",
    "    r2 = r2_score(y_true, y_pred, sample_weight=sample_weight,\n",
    "                  multioutput=multioutput)\n",
    "    r = (np.sign(r2)*np.sqrt(np.abs(r2)))\n",
    "    if r <= -1:\n",
    "        return -1\n",
    "    else:\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "efnVpoVi6LS7"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dv72AHnByllk"
   },
   "outputs": [],
   "source": [
    "class Observation(object):\n",
    "    def __init__(self, train, target, features):\n",
    "        self.train = train\n",
    "        self.target = target\n",
    "        self.features = features\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "    def __init__(self):\n",
    "        with pd.HDFStore(\"/content/drive/My Drive/train.h5\", \"r\") as hfdata:\n",
    "            self.timestamp = 0\n",
    "            fullset = hfdata.get(\"train\")\n",
    "            self.unique_timestamp = fullset[\"timestamp\"].unique()\n",
    "            # Get a list of unique timestamps\n",
    "            # use the first half for training and\n",
    "            # the second half for the test set\n",
    "            n = len(self.unique_timestamp)\n",
    "            i = int(n/2)\n",
    "            timesplit = self.unique_timestamp[i]\n",
    "            self.n = n\n",
    "            self.unique_idx = i\n",
    "            self.train = fullset[fullset.timestamp < timesplit]\n",
    "            self.test = fullset[fullset.timestamp >= timesplit]\n",
    "\n",
    "            # Needed to compute final score\n",
    "            self.full = self.test.loc[:, ['timestamp', 'y']]\n",
    "            self.full['y_hat'] = 0.0\n",
    "            self.temp_test_y = None\n",
    "\n",
    "    def reset(self):\n",
    "        timesplit = self.unique_timestamp[self.unique_idx]\n",
    "\n",
    "        self.unique_idx = int(self.n / 2)\n",
    "        self.unique_idx += 1\n",
    "        subset = self.test[self.test.timestamp == timesplit]\n",
    "\n",
    "        # reset index to conform to how kagglegym works\n",
    "        target = subset.loc[:, ['id', 'y']].reset_index(drop=True)\n",
    "        self.temp_test_y = target['y']\n",
    "\n",
    "        target.loc[:, 'y'] = 0.0  # set the prediction column to zero\n",
    "\n",
    "        # changed bounds to 0:110 from 1:111 to mimic the behavior\n",
    "        # of api for feature\n",
    "        features = subset.iloc[:, :110].reset_index(drop=True)\n",
    "\n",
    "        observation = Observation(self.train, target, features)\n",
    "        return observation\n",
    "\n",
    "    def step(self, target):\n",
    "        timesplit = self.unique_timestamp[self.unique_idx-1]\n",
    "        # Since full and target have a different index we need\n",
    "        # to do a _values trick here to get the assignment working\n",
    "        y_hat = target.loc[:, ['y']]\n",
    "        self.full.loc[self.full.timestamp == timesplit, ['y_hat']] = y_hat._values\n",
    "\n",
    "        if self.unique_idx == self.n:\n",
    "            done = True\n",
    "            observation = None\n",
    "            reward = r_score(self.temp_test_y, target.loc[:, 'y'])\n",
    "            score = r_score(self.full['y'], self.full['y_hat'])\n",
    "            info = {'public_score': -score}\n",
    "        else:\n",
    "            reward = r_score(self.temp_test_y, target.loc[:, 'y'])\n",
    "            done = False\n",
    "            info = {}\n",
    "            timesplit = self.unique_timestamp[self.unique_idx]\n",
    "            self.unique_idx += 1\n",
    "            subset = self.test[self.test.timestamp == timesplit]\n",
    "\n",
    "            # reset index to conform to how kagglegym works\n",
    "            target = subset.loc[:, ['id', 'y']].reset_index(drop=True)\n",
    "            self.temp_test_y = target['y']\n",
    "\n",
    "            # set the prediction column to zero\n",
    "            target.loc[:, 'y'] = 0\n",
    "\n",
    "            # column bound change on the subset\n",
    "            # reset index to conform to how kagglegym works\n",
    "            features = subset.iloc[:, 0:110].reset_index(drop=True)\n",
    "\n",
    "            observation = Observation(self.train, target, features)\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Environment()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trVF22fqnl70"
   },
   "outputs": [],
   "source": [
    "#9 Agent Environment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "WBlXpYBtmSAM",
    "outputId": "61685106-f8cd-44e8-d11d-d7fa2cd111c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968\n",
      "806298\n"
     ]
    }
   ],
   "source": [
    "#10 test it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4qbw2OJ4gLT"
   },
   "source": [
    "How could this RL framework be extended? Q learning! But we need a real-time API or simulated environment where actions effect the enviroment state\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Kao-Shing_Hwang/publication/220776448/figure/fig1/AS:394068661161984@1470964698231/The-Q-Learning-Algorithm-6.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Kaggle_demo_LIVE_TwoSigma.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
